<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
    <title>APTS</title>
</head>
<body>
<div class="bg-image synopsis_bg_img" >

    <nav class="mask">
        <a href="index.html"><img class="logo" src="images/logo.png" alt="logo"></a>
        <a href="index.html"><h class="title">APTS</h></a>
        <ul class="list">
          <li><a href="Synopsis.html">Synopsis</a></li>
          <li><a href="#">About Us</a></li>
          <li><a href="#">Contact</a></li>
        </ul>
    </nav>
         
    <div class="parent_synopsis_box">

        <div class="primary_synopsis_box"> 
            <div class="synopsis_content_box ">
                <h2 class="synopsis_topic">TEAM AND GUIDE</h2>
                <div class="synopsis_content scroll">

                    <!-- For Team and Guide -->
                    <div class="team center">
                        <h3 class="blue">Team Details:</h3>
                            <p> <br>1451 Vikas Puniya (PM) <br>
                                    1418 Harshvardhan <br>
                                    1432 Prakash Kumar Dubey <br><br><br>
                            </p>
                        <h3 class="blue"><br>Project Guide:</h3><br>
                        <p>Dr. Avinash Patil</p><br>
                        <h3 class="blue"><br>College:</h3><br>
                        <p>Army Institute of Technology, Pune</p> <br><br><br>
                    </div>

                    <!-- For  INTRODUCTION-->
                    <div class="introduction hide">
                            <p> The "Automated Precision Targeting System" is an ambitious project that seeks to harness the power of robotics, artificial intelligence, and advanced sensor technologies to revolutionize the way targets are acquired and engaged. In a rapidly evolving technological landscape, the need for precision, accuracy, and speed in targeting systems has never been more critical. Whether for military defence, law enforcement, or other applications, the ability to engage targets efficiently and ethically with minimal human intervention is a compelling goal. <br><br>
                                This project endeavours to create a cutting-edge robotic mechanism capable of detecting, tracking, and precisely aiming at targets with a high degree of accuracy and speed. Leveraging state-of-the-art computer vision, machine learning algorithms, and sensor fusion, this system aims to navigate complex environments, identify potential threats, and engage them with the utmost precision. <br><br>
                                In this journey, we will delve into the realms of robotics, automation, computer vision, and artificial intelligence, exploring the latest developments, ethical considerations, and practical applications that define the future of automated precision targeting. With a commitment to safety, ethics, and responsible technology deployment, we embark on a mission to redefine what is possible in the world of targeting systems. <br>
                                
                            </p>
                        <h3 class="blue"><br>Primary Reason to Choose the Problem: <br></h3> <br>
                        <p>The primary reason for selecting this problem is the increasing demand for advanced automation in security, surveillance, and training applications. Current methods often rely on human operators and lack the speed, accuracy, and continuous monitoring capabilities that an APTS can provide. Through inputs from currently ongoing conflicts between Russia and Ukraine, we see the use of drone dropping of grenades and other ammunitions over enemy combatants. The success ratio is around the range of 30-40% mainly due to parameters like incorrect alignment, novice inexperience of operator, no proper targeting mechanism, wind, canopy, natural obstructions and unaccounted aero dynamicity of projectile to name a few. By addressing this problem, we aim to enhance situational awareness, improve security measures, and create a platform for realistic training scenarios. Furthermore, the project offers an opportunity for interdisciplinary learning and innovation in the fields of computer vision, robotics, and human-machine interaction.</p>
                    </div>
                    
                    <!-- For OBJECTIVE -->
                    <div class="objective hide">
                        
                            <p> <br>Design, develop, and implement a fully functional Automated Precision Targeting System (APTS) capable of the following:<br><br>
                            </p>
                            
                            <ul>
                                <li><b class="blue">1.	Real-time Human Detection and Tracking:</b> Utilizing computer vision techniques to accurately detect and continuously track human targets within a given environment using input from a camera.</li><br>
                                <li><b class="blue">2.	Precision Aiming Mechanism:</b> Implementing a motorized or servo-based aiming system to align a simulated weapon or device with the coordinates of the detected human targets.</li><br>
                                <li><b class="blue">3.	Sequential Targeting:</b> Enabling the system to sequentially target and simulate precision firing actions on each detected human in the frame, one by one.</li><br>
                                <li><b class="blue">4.	Safety and Ethics:</b> Prioritizing safety by ensuring that the simulated weapon or device does not pose any harm. Implementing ethical considerations in terms of privacy and responsible use.</li><br>
                                <li><b class="blue">5.	User Interaction:</b> Providing a user interface or control system for remote operation and monitoring of the APTS.</li><br>
                                <li><b class="blue">6.	Versatility and Applicability:</b> Creating a system that can be adapted for various scenarios, including security, surveillance, and training, while remaining safe and ethical.</li>
                            </ul>
                    </div>

                    <!-- For SCOPE  -->
                    <div class="scope hide">
                        <ul>
                            <li><b class="blue">1.	Technical Scope:</b> <br><br>
                                a)	<u><b>Computer Vision Development:</b></u> Developing and implementing advanced computer vision algorithms for real-time human detection and tracking. <br>
                                b)	<u><b>Robotics and Motor Control:</b></u> Designing and building the precision aiming mechanism, involving motors or servos for adjusting the orientation of the simulated weapon or device. <br>
                                c)	<u><b>User Interface (UI):</b></u> Creating a user-friendly UI for remote control and monitoring of the APTS. <br>
                                </li><br>
                                <li><b class="blue">2.	Practical Scope:</b> <br><br>
                                    a)	<u><b>Security Applications:</b></u> Deployment of the APTS in security scenarios, such as perimeter defense, to enhance surveillance and threat response. <br>
                                    b)	<u><b>Training Simulations:</b></u> Application of the system in training simulations for military or law enforcement personnel to create realistic training environments. <br>
                                    c)	<u><b>Search and Rescue:</b></u> Potential use in search-and-rescue operations for locating and tracking individuals in emergency situations. <br>
                                    </li><br>
                                <li><b class="blue">3.	Interdisciplinary Learning:</b> <br><br>
                                    a)	Providing an opportunity for interdisciplinary learning by combining computer science, robotics, electrical engineering, and ethical considerations in technology development.
                                    </li>
                        </ul>
                    </div>

                    <!-- For WORKING METHODOLGY -->
                    <div class="working hide">
                        
                            <p> <ul>
                                    The working methodology for the project involves several key components and steps:<br><br>
                                    <li>
                                        <b class="blue">1.	Data Acquisition and Input:</b> <br>
                                        •	The project begins with the acquisition of data from a camera or multiple cameras strategically positioned to capture the target environment.
                                        •	The camera(s) provide a continuous video feed, which serves as the primary input for the APTS.<br><br>
                                    </li>
                                    <li>
                                       <b class="blue"> 2.	Computer Vision-Based Detection and Tracking:</b><br>
                                        •	Advanced computer vision algorithms are applied to the video feed to detect and identify human targets within the frame.
                                        •	Techniques such as YOLO (You Only Look Once) or SSD (Single Shot MultiBox Detector) can be used for efficient object detection.
                                        •	Detected targets are then tracked over successive frames to maintain their coordinates.<br><br>
                                    </li>
                                    <li>
                                        <b class="blue">3.	Motor Control for Precision Aiming:</b><br>
                                        •	A precision aiming mechanism, typically involving motors or servos, is employed to control the orientation of a simulated weapon or device.
                                        •	Based on the coordinates of the detected human targets, the system calculates the necessary adjustments to align the simulated weapon with the target.<br><br>
                                    </li>
                                    <li>
                                        <b class="blue">4.	Safety Measures:</b><br>
                                        •	Safety measures are implemented to ensure that the simulated weapon or device does not pose any harm to friendly forces or animals.
                                        •	These measures may include physical constraints on the simulated weapon's movement, software-based safety checks, and emergency stop mechanisms.<br><br>
                                    </li>
                                    <li>
                                        <b class="blue">5.	Sequential Targeting Algorithm:</b><br>
                                        •	An algorithm is developed to determine the sequence in which the APTS should target detected humans.
                                        •	This algorithm ensures that targets are engaged one by one, with precise aiming and simulated firing actions.<br><br>
                                    </li>
                                    <li>
                                        <b class="blue">6.	User Interaction:</b><br>
                                        •	Depending on the project's design, a user interface (UI) or remote-control system may be provided to allow users to monitor the system's operation and make adjustments.
                                        •	The UI can display the video feed, target information, and system status.
                                    </li>
                                    
                                </ul>
                            </p>
                        
                    </div>

                    <!-- For HARDWARE AND SOFTWARE -->
                    <div class="hardware hide">
                        
                            <p> <h3 class="blue">Hardware Components:</h3> <br> <br>
                                <ul>
                                    <b class="light">1.	Camera(s):</b> <br> 
                                    <li>i.	For night vision: Workswell WIRIS ( 680 grams,  digital camera resolution 16 MP, resolution 800×600  and thermal sensitivity of 40 mK,  focal length within interval of 129.0 mm to 4.3 mm and it equals up to 30x optical ZOOM, 30 Hz thermal frame rate IP66, up to 1500m Range finder, GPS sensor and tagging, gimble stable)</li>
                                    <li>
                                        ii.	For Digital vision: RunCam Hybrid 2 ( 18 grams, 145 degree FOV, 4K – 120fps support, resolution 720*576, 140 mA@12 V or 480mA@5V, super low latency)
                                        
                                    </li>
                                    100 meters range operable cameras:
                                    <li>i.	FLIR Vue Pro: 118-142 grams, 336x256, 640x512, thermal sensitivity (50 mK,  30 Hz frame rate , range 1 kilometre in ideal conditions</li>
                                    <li>ii.	DJI Zenmuse XT2: 640, 640x512 and 336x256 resolution options, Typically (50 mK, up to 30 Hz , range 1 kilometre in ideal conditions))</li>
                                </ul> <br> 
                                <ul>
                                    
                                    <b class="light">2.	Precision Aiming Mechanism:</b><br>
                                    •	12 V Motors or servos are used to control the orientation of the simulated weapon or device. (Johnson Geared Motor 500 RPM 12v dc, servo motor)
                                </ul><br>
                                <ul> 
                                    <b class="light">3.	Computer or Microcontroller:</b><br>
                                    •	A computer or microcontroller serves as the central processing unit for running the software, processing video data, and controlling the hardware components.
                                    •	NVIDIA Jetson, Raspberry Pi or a dedicated PC. <br><br>
                                    <b class="light">4.	Simulated Weapon or Device:</b><br>
                                    •	Depending on the project's requirements, this could be a simulated gun (Airsoft pistol/rifle), laser pointer, or combat AR / Pistol. <br><br>
                               <b class="light"> 5.	Power Supply:</b><br>
                                •	Adequate power supplies are needed for the camera, motors, and other electronic components. (12V adaptor, Orange 12 V battery) <br><br>
                               <b class="light"> 6.	Sensors:</b><br>
                                •	Sensors like proximity sensors or accelerometers can enhance the safety of the system by detecting obstacles or sudden movements. <br>
                                •	Johnson Geared Motor 500 RPM 12v dc, ADXL345 Tripple Axis Accelerometer <br><br>
                                <b class="light">7.	User Interface Components:</b><br>
                                •	Smartphone or digital screen for controls and mode selection <br><br><br><br>
                            </ul>
                                
                                <h3 class="blue">Software Components:</h3> <br>
                                <ul>
                                    
                                    <li>
                                        1.	<b class="light">Computer Vision Libraries:</b> Open-source computer vision libraries like OpenCV are crucial for image and video processing, object detection, and tracking.
                                        <br><br>
                                    </li>
                                    <li>
                                        2.	<b class="light">Machine Learning Models:</b> Pre-trained machine learning models for object detection (e.g., YOLO, SSD) can be utilized to identify and locate human targets in real-time.
                                        <br><br>
                                    </li>
                                    
                                    <li>
                                        3.	<b class="light">Motor Control Software:</b> Software for controlling the motors or servos, adjusting the orientation of the simulated weapon based on target coordinates. ( NVIDIA Jetson software stack, LabNet)
                                        <br><br>
                                    </li>
                                    <li>
                                        4.	<b class="light">User Interface:</b> If a user interface is part of the project, software for designing and implementing the UI is required. This could involve frameworks like PyQt or web-based interfaces.
                                        <br><br>
                                    </li>
                                </ul>
                            </p>
                        
                    </div>

                    <!-- For LIMITATIONS -->
                    <div class="limitations hide">
                            <p> 1.	<b class="blue">Safety Risks:</b>
                                Safety is a paramount concern. The project must ensure that no harm can come to friendly forces. <br><br>
                                2.	<b class="blue">Accuracy and Precision:</b>
                                Achieving accurate and precise targeting is challenging. Factors like environmental conditions, camera quality, and calibration can affect the system's accuracy. <br><br>
                                3.	<b class="blue">Target Recognition:</b>
                                Accurately detecting and recognizing humans in real-time can be challenging due to factors like varying lighting conditions, occlusions, and different body postures. <br><br>
                                4.	<b class="blue">Response Time:</b>
                                The system's response time is critical, especially in dynamic environments. Delays in targeting and firing could render the system ineffective or impractical. <br><br>
                                5.	<b class="blue">Cost and Resources:</b>
                                Building a sophisticated automated targeting system can be expensive and resource-intensive, requiring specialized hardware, software, and expertise. <br><br>
                                6.	<b class="blue">Privacy and comptonization of network Concerns:</b>
                                The use of cameras for surveillance and targeting uses network connectivity that can be disrupted by electronic warfare. <br><br>
                                7.	<b class="blue">Maintenance and Calibration:</b>
                                The system would require regular maintenance and calibration to ensure it functions correctly over time. Neglecting these aspects can lead to performance degradation. <br><br>
                                8.	<b class="blue">Environmental Factors:</b>
                                Environmental factors such as weather conditions (e.g., rain, fog) can affect the system's performance. Robustness to varying conditions is essential. <br><br>
                                9.	<b class="blue">Regulatory Compliance:</b>
                                Depending on the project's location and intended use, it may need to comply with various regulations and obtain permits, which can be a complex process. <br><br>
                            </p>
                        
                    </div>

                    <!-- For REFRENCES -->
                    <div class="references hide">
                        
                            <p>
                                <ul>
                                    <li> 1.	S. S. Ali, M. F. Zafar and M. Tayyab, "<b class="blue">Detection and Recognition of Human in Videos Using Adaptive Method and Neural Net</b>" 2009 International Conference of Soft Computing and Pattern Recognition, Malacca, Malaysia, 2009, pp. 604-609, doi: 10.1109/SoCPaR.2009.119. <br>
                                    <i>(In the study, the authors propose a modified framework for detecting and recognizing moving individuals in dynamic video environments. Their method combines an average background model with adaptive threshold selection based on Gaussian distribution, enhancing background modelling and system adaptability. Feature extraction utilizes a robust human model, and recognition employs a backpropagation neural network classifier, demonstrating the effectiveness of their system.)</i> <br><br>
                                    </li>
                                    <li>
                                    2.	S. K. Sharma, R. Agrawal, S. Srivastava and D. K. Singh, "<b class="blue">Review of human detection techniques in night vision</b>" 2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET), Chennai, India, 2017, pp. 2216-2220, doi: 10.1109/WiSPNET.2017.8300153. <br>
                                    <i>(The paper reviews two algorithms for human detection in night vision scenarios, crucial for surveillance applications. The hot-spot algorithm applies black body radiation theory, while the background subtraction algorithm leverages difference images. Experimentation and analysis of these approaches showcase their potential in night vision-based surveillance.)</i> <br><br>
                                    </li>
                                    <li>
                                    3.	P. A. Dhulekar, S. T. Gandhe, N. Sawale, V. Shinde and S. Khute, "<b class="blue">Surveillance System for Detection of Suspicious Human Activities at War Field</b>" 2018 International Conference On Advances in Communication and Computing Technology (ICACCT), Sangamner, India, 2018, pp. 357-360, doi: 10.1109/ICACCT.2018.8529632. <br>
                                    <i>(The research focuses on enhancing war field surveillance using intelligent systems, utilizing a Raspberry Pi processor and night vision camera. This system aims to identify enemy presence in low-light conditions, reducing human casualties and border infiltrations. It employs optical flow algorithms for movement detection, showing promise in defence applications.)</i> <br><br>
                                    </li>
                                    4.	J. Zhao, J. Li, Z. Huang, G. Bao, W. He and Z. Lu, "<b class="blue">Research on Projectile Launching of Robot Gimble Based on Kalman Filter and ADRC Control</b>" 2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM), Shenzhen, China, 2020, pp. 31-37, doi: 10.1109/ICARM49381.2020.9195368. <br>
                                    <i>(The study addresses the challenge of accurately hitting dynamic targets with a projectile launching mechanism. The proposed system utilizes image processing, Kalman filtering, and ADRC control algorithms to predict target motion and control the shot hit system, achieving accurate projectile strikes on moving targets within specific distances.)</i>
                                    </li>
                                </ul>
                            </p>
                        
                    </div>

                </div>
            </div>
        </div>

        <div class="secondary_synopsis_box">
            <button onclick="hideshow(1)" class="secondary_synopsis_box_btn1 synopsis_button"><p><b>TEAM AND GUIDE</b></p></button>
            <button onclick="hideshow(2)" class="secondary_synopsis_box_btn2 synopsis_button"><p><b>INTRODUCTION</b></p></button>
            <button onclick="hideshow(3)" class="secondary_synopsis_box_btn3 synopsis_button"><p><b>OBJECTIVE</b></p></button>
            <button onclick="hideshow(4)" class="secondary_synopsis_box_btn4 synopsis_button"><p><b>SCOPE</b></p></button>
            <button onclick="hideshow(5)" class="secondary_synopsis_box_btn5 synopsis_button"><p><b>WORKING METHODOLGY</b></p></button>
            <button onclick="hideshow(6)" class="secondary_synopsis_box_btn6 synopsis_button"><p><b>HARDWARE AND SOFTWARE</b></p></button>
            <button onclick="hideshow(7)" class="secondary_synopsis_box_btn7 synopsis_button"><p><b>LIMITATIONS</b></p></button>
            <button onclick="hideshow(8)" class="secondary_synopsis_box_btn8 synopsis_button"><p><b>REFERENCES</b></p></button>
        </div>

    </div>

</div> 
    <script async src="capture.js"></script>
</body>
</html>
